{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "pKNGPT2-DYN",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tegee/gpt-2/blob/master/pKNGPT2_DYN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iU1oq8-nxjGj"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5eq_s3feju8",
        "outputId": "90681b15-ad4e-465f-8363-5fd328a8a6f4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrTTr6NL3FR3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27eb74ed-bdc1-49e4-bc59-9615e38a4409"
      },
      "source": [
        "%pip install tensorflow==1.13.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.13.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/db/d3/651f95288a6cd9094f7411cdd90ef12a3d01a268009e0e3cd66b5c8d65bd/tensorflow-1.13.2-cp36-cp36m-manylinux1_x86_64.whl (92.6MB)\n",
            "\u001b[K     |████████████████████████████████| 92.6MB 43kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (0.36.1)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 46.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.18.5)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 42.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (3.12.4)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.34.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (0.10.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (0.3.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.1.2)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.3.3)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/03/b7e605db4a57c0f6fba744b11ef3ddf4ddebcada35022927a2b5fc623fdf/mock-4.0.3-py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.2) (50.3.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.2) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.4.0)\n",
            "Installing collected packages: tensorboard, mock, tensorflow-estimator, keras-applications, tensorflow\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed keras-applications-1.0.8 mock-4.0.3 tensorboard-1.13.1 tensorflow-1.13.2 tensorflow-estimator-1.13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZJwuWIJsQI_",
        "outputId": "e31326b0-bd99-49e8-9b4b-4d037aa278ef"
      },
      "source": [
        "import os\n",
        "import pdb\n",
        "%pip install tensorflow==1.13\n",
        "proj_folder = '/content/gpt-2'\n",
        "git_src = 'https://github.com/openai/gpt-2' \n",
        "if not os.path.exists(proj_folder):\n",
        "  !git clone $git_src\n",
        "else:\n",
        "  print('existed: %s' % proj_folder)\n",
        "  os.chdir(proj_folder)  \n",
        "  !git pull origin master\n",
        "\n",
        "os.chdir(proj_folder)\n",
        "!pip3 install -r requirements.txt\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "print('tf version: %s' % tf.__version__)\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if 'GPU' in device_name:\n",
        "  print('GPU ready: %s' % device_name) \n",
        "  GPU_FLAG = True\n",
        "else:\n",
        "  print('CPU only.....')    \n",
        "\n",
        "src_path = '/content/gpt-2/src'\n",
        "if src_path not in sys.path:\n",
        "  sys.path += [src_path]\n",
        "\n",
        "os.chdir(proj_folder)\n",
        "model_name= '345M'  \n",
        "if os.path.exists(os.path.join('models', model_name)) == False:\n",
        "  print('download model %s....' % model_name)\n",
        "  !PYTHONPATH=src; python ./download_model.py $model_name\n",
        "else:\n",
        "  print('existed: model %s' % model_name)    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==1.13 (from versions: 0.12.1, 1.0.0, 1.0.1, 1.1.0rc0, 1.1.0rc1, 1.1.0rc2, 1.1.0, 1.2.0rc0, 1.2.0rc1, 1.2.0rc2, 1.2.0, 1.2.1, 1.3.0rc0, 1.3.0rc1, 1.3.0rc2, 1.3.0, 1.4.0rc0, 1.4.0rc1, 1.4.0, 1.4.1, 1.5.0rc0, 1.5.0rc1, 1.5.0, 1.5.1, 1.6.0rc0, 1.6.0rc1, 1.6.0, 1.7.0rc0, 1.7.0rc1, 1.7.0, 1.7.1, 1.8.0rc0, 1.8.0rc1, 1.8.0, 1.9.0rc0, 1.9.0rc1, 1.9.0rc2, 1.9.0, 1.10.0rc0, 1.10.0rc1, 1.10.0, 1.10.1, 1.11.0rc0, 1.11.0rc1, 1.11.0rc2, 1.11.0, 1.12.0rc0, 1.12.0rc1, 1.12.0rc2, 1.12.0, 1.12.2, 1.12.3, 1.13.0rc0, 1.13.0rc1, 1.13.0rc2, 1.13.1, 1.13.2, 1.14.0rc0, 1.14.0rc1, 1.14.0, 1.15.0rc0, 1.15.0rc1, 1.15.0rc2, 1.15.0rc3, 1.15.0, 1.15.2, 1.15.3, 1.15.4, 2.0.0a0, 2.0.0b0, 2.0.0b1, 2.0.0rc0, 2.0.0rc1, 2.0.0rc2, 2.0.0, 2.0.1, 2.0.2, 2.0.3, 2.1.0rc0, 2.1.0rc1, 2.1.0rc2, 2.1.0, 2.1.1, 2.1.2, 2.2.0rc0, 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4, 2.2.0, 2.2.1, 2.3.0rc0, 2.3.0rc1, 2.3.0rc2, 2.3.0, 2.3.1, 2.4.0rc0, 2.4.0rc1, 2.4.0rc2, 2.4.0rc3, 2.4.0rc4)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for tensorflow==1.13\u001b[0m\n",
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 233 (delta 0), reused 1 (delta 0), pack-reused 230\u001b[K\n",
            "Receiving objects: 100% (233/233), 4.38 MiB | 15.10 MiB/s, done.\n",
            "Resolving deltas: 100% (122/122), done.\n",
            "Collecting fire>=0.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/a7/0e22e70778aca01a52b9c899d9c145c6396d7b613719cd63db97ffa13f2f/fire-0.3.1.tar.gz (81kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 4.2MB/s \n",
            "\u001b[?25hCollecting regex==2017.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
            "\u001b[K     |████████████████████████████████| 604kB 10.9MB/s \n",
            "\u001b[?25hCollecting requests==2.21.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/e3/20f3d364d6c8e5d2353c72a67778eb189176f08e873c9900e10c0287b84b/requests-2.21.0-py2.py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.5MB/s \n",
            "\u001b[?25hCollecting tqdm==4.31.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.1.0)\n",
            "Collecting idna<2.9,>=2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2020.12.5)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.24.3)\n",
            "Building wheels for collected packages: fire, regex\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.3.1-py2.py3-none-any.whl size=111007 sha256=b21335b9ae2e1c2e7e8d92ebe949bfde40803b7a7e7b076f521ce0b956e8fe7e\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/61/df/768b03527bf006b546dce284eb4249b185669e65afc5fbb2ac\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2017.4.5-cp36-cp36m-linux_x86_64.whl size=533203 sha256=7786911a2ca3f8283881cbc78a170b10a65146c2cf7ca4eb4438e84944a8ec49\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
            "Successfully built fire regex\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement tqdm<5.0.0,>=4.38.0, but you'll have tqdm 4.31.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.21.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement tqdm>=4.36.1, but you'll have tqdm 4.31.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: fire, regex, idna, requests, tqdm\n",
            "  Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "Successfully installed fire-0.3.1 idna-2.8 regex-2017.4.5 requests-2.21.0 tqdm-4.31.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tf version: 1.13.2\n",
            "CPU only.....\n",
            "download model 345M....\n",
            "Fetching checkpoint: 1.00kit [00:00, 819kit/s]                                                      \n",
            "Fetching encoder.json: 1.04Mit [00:00, 4.25Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 833kit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:26, 53.8Mit/s]                                 \n",
            "Fetching model.ckpt.index: 11.0kit [00:00, 5.95Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 927kit [00:00, 5.78Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 2.69Mit/s]                                                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9C9lZ6jgrCo"
      },
      "source": [
        "\n",
        "import requests\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    def get_confirm_token(response):\n",
        "        for key, value in response.cookies.items():\n",
        "            if key.startswith('download_warning'):\n",
        "                return value\n",
        "\n",
        "        return None\n",
        "\n",
        "    def save_response_content(response, destination):\n",
        "        CHUNK_SIZE = 32768\n",
        "\n",
        "        with open(destination, \"wb\") as f:\n",
        "            for chunk in response.iter_content(CHUNK_SIZE):\n",
        "                if chunk: # filter out keep-alive new chunks\n",
        "                    f.write(chunk)\n",
        "\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "    session = requests.Session()\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "    save_response_content(response, destination)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nI-rkoeYTWFP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa8d8fd0-7309-4719-f6b1-660f6b053945"
      },
      "source": [
        "# donwload fine-tuned model for KN\n",
        "\n",
        "ckpt_path = '/content/drive/MyDrive/finetuned_models/checkpoint/KNGPT2P'\n",
        "if os.path.exists(ckpt_path):\n",
        "  print('Existed: %s' % ckpt_path)\n",
        "  !ls $ckpt_path\n",
        "else:\n",
        "  os.mkdir(ckpt_path)\n",
        "  os.chdir(ckpt_path)\n",
        "  print('Downloading files to %s....' % ckpt_path)\n",
        "  download_file_from_google_drive('10Qc8SIuwq6w6guwDnpdNzKH24NxZ3oG-', 'checkpoint')\n",
        "  download_file_from_google_drive('1KDO8ikS5IJqo1S6zfbOnFae3-zS6-8f0', 'model-1500.meta')\n",
        "  download_file_from_google_drive('1Z0PnW0BHFApZBBWL3SGDzwgFme1mwFwi', 'model-1500.data-00000-of-00001')\n",
        "  download_file_from_google_drive('1ipOdKVOUM8h45njLLtbhJFbfqTNFLNq_', 'model-1500.index')\n",
        "  !ls -al \n",
        "  print('Download: ok')\n",
        "os.chdir(proj_folder)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Existed: /content/drive/MyDrive/finetuned_models/checkpoint/KNGPT2P\n",
            "checkpoint    events.out.tfevents.1607357806.8adf29f9aac9  model-1500.index\n",
            "counter       hparams.json\t\t\t\t   model-1500.meta\n",
            "encoder.json  model-1500.data-00000-of-00001\t\t   vocab.bpe\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiBKgo1SsQWu",
        "outputId": "6938f0e2-0346-4b88-e9fd-7e11bc3a86ff"
      },
      "source": [
        "# the following is my enchancement based on: \n",
        "# https://github.com/openai/gpt-2/blob/master/src/sample.py\n",
        "# https://github.com/openai/gpt-2/blob/master/src/interactive_conditional_samples.py\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import model, sample, encoder\n",
        "\n",
        "def generate_text(seed_text, sess, output, context, nsamples, batch_size):\n",
        "    generated = 0\n",
        "    for _ in range(nsamples // batch_size):\n",
        "      out = sess.run(output, feed_dict={\n",
        "          context: [context_tokens for _ in range(batch_size)]\n",
        "      })[:, len(context_tokens):]\n",
        "      for i in range(batch_size):\n",
        "        generated += 1\n",
        "        text = enc.decode(out[i])\n",
        "        text = till_end_of_text(text)\n",
        "        text = seed_text + ' ' + text\n",
        "        print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
        "        text = text.replace(tag_start, '').strip()\n",
        "        span_text = text.replace(' @@@ ','\\n    ')\n",
        "        print(span_text)\n",
        "    print(\"=\" * 80)\n",
        "    print('\\n')\n",
        "\n",
        "def dynamic_kp_logits(logits, top_kp):\n",
        "    k = 1000 # observe probability of the top n \n",
        "    probs_logits = tf.nn.softmax(logits)\n",
        "    k_probs, _ = tf.nn.top_k(probs_logits, k=k)\n",
        "    k_probs = tf.squeeze(k_probs)\n",
        "    probs_max = tf.reduce_max(k_probs)\n",
        "    k_threshold = tf.multiply(probs_max, top_kp)\n",
        "    probs_mask = tf.to_int32(k_probs >= k_threshold)\n",
        "    num_of_k = tf.count_nonzero(probs_mask, dtype=tf.float32) \n",
        "\n",
        "    values, _ = tf.nn.top_k(logits, k=tf.cast(num_of_k, dtype=tf.int32))\n",
        "    min_values = values[:, -1, tf.newaxis]\n",
        "    result_logits = tf.where(\n",
        "        logits < min_values,\n",
        "        tf.ones_like(logits, dtype=logits.dtype) * -1e10,\n",
        "        logits,)\n",
        "\n",
        "    return result_logits\n",
        "\n",
        "def sample_sequence_kp(*, hparams, length, start_token=None, batch_size=None, \n",
        "                       context=None, temperature=1, top_kp=0.1):\n",
        "    if start_token is None:\n",
        "        assert context is not None, 'Specify exactly one of start_token and context!'\n",
        "    else:\n",
        "        assert context is None, 'Specify exactly one of start_token and context!'\n",
        "        context = tf.fill([batch_size, 1], start_token)\n",
        "\n",
        "    def step(hparams, tokens, past=None):\n",
        "        lm_output = model.model(hparams=hparams, X=tokens, past=past, reuse=tf.AUTO_REUSE)\n",
        "\n",
        "        logits = lm_output['logits'][:, :, :hparams.n_vocab]\n",
        "        presents = lm_output['present']\n",
        "        presents.set_shape(model.past_shape(hparams=hparams, batch_size=batch_size))\n",
        "        return {\n",
        "            'logits': logits,\n",
        "            'presents': presents,\n",
        "        }\n",
        "\n",
        "    with tf.name_scope('sample_sequence'):\n",
        "        def body(past, prev, output):\n",
        "            next_outputs = step(hparams, prev, past=past)\n",
        "            logits = next_outputs['logits'][:, -1, :]  / tf.to_float(temperature)\n",
        "            logits = dynamic_kp_logits(logits, top_kp)\n",
        "            samples = tf.multinomial(logits, num_samples=1, output_dtype=tf.int32)\n",
        "            return [\n",
        "                next_outputs['presents'] if past is None else tf.concat([past, next_outputs['presents']], axis=-2),\n",
        "                samples,\n",
        "                tf.concat([output, samples], axis=1)\n",
        "            ]\n",
        "\n",
        "        past, prev, output = body(None, context, context)\n",
        "\n",
        "        def cond(*args):\n",
        "            return True\n",
        "\n",
        "        _, _, tokens = tf.while_loop(\n",
        "            cond=cond, body=body,\n",
        "            maximum_iterations=length - 1,\n",
        "            loop_vars=[\n",
        "                past,\n",
        "                prev,\n",
        "                output\n",
        "            ],\n",
        "            shape_invariants=[\n",
        "                tf.TensorShape(model.past_shape(hparams=hparams, batch_size=batch_size)),\n",
        "                tf.TensorShape([batch_size, None]),\n",
        "                tf.TensorShape([batch_size, None]),\n",
        "            ],\n",
        "            back_prop=False,\n",
        "        )\n",
        "\n",
        "        return tokens\n",
        "\n",
        "def till_end_of_text(text):\n",
        "  tag_end = '<|eos|>'\n",
        "  result = ''\n",
        "  pos2 = text.find(tag_end)\n",
        "  if pos2 == -1:\n",
        "    print(\"end of texts not detected\")\n",
        "  elif pos2 == 0:\n",
        "    result = '(end of sentence )'\n",
        "  else:\n",
        "    result = text[:pos2].strip()\n",
        "\n",
        "  return result\n",
        "\n",
        "# main program\n",
        "model_name='345M'\n",
        "seed=None\n",
        "nsamples=1\n",
        "batch_size=1\n",
        "length=None\n",
        "temperature=1\n",
        "top_k=40\n",
        "top_p=0.9\n",
        "top_kp=0.1\n",
        "\n",
        "tag_start = '<|sos|>'\n",
        "\n",
        "models_dir = 'models'\n",
        "models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n",
        "if batch_size is None:\n",
        "  batch_size = 1\n",
        "assert nsamples % batch_size == 0\n",
        "\n",
        "enc = encoder.get_encoder(model_name, models_dir)\n",
        "hparams = model.default_hparams()\n",
        "with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:\n",
        "  hparams.override_from_dict(json.load(f))\n",
        "\n",
        "if length is None:\n",
        "  length = hparams.n_ctx // 2\n",
        "elif length > hparams.n_ctx:\n",
        "  raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
        "\n",
        "with tf.Session(graph=tf.Graph()) as sess:\n",
        "  context = tf.placeholder(tf.int32, [batch_size, None])\n",
        "  np.random.seed(seed)\n",
        "  tf.set_random_seed(seed)\n",
        "\n",
        "  # original sampling in GPT-2\n",
        "  output_top_k = sample.sample_sequence(\n",
        "    hparams=hparams, length=length,\n",
        "    context=context,\n",
        "    batch_size=batch_size,\n",
        "    temperature=temperature, top_k=top_k, top_p=1)\n",
        "  output_top_p = sample.sample_sequence(\n",
        "    hparams=hparams, length=length,\n",
        "    context=context,\n",
        "    batch_size=batch_size,\n",
        "    temperature=temperature, top_k=0, top_p=top_p)\n",
        "  \n",
        "  # a different sampling in our research\n",
        "  output_top_kp = sample_sequence_kp(\n",
        "    hparams=hparams, length=length,\n",
        "    context=context,\n",
        "    batch_size=batch_size,\n",
        "    temperature=temperature, top_kp=top_kp)\n",
        "\n",
        "  saver = tf.train.Saver()\n",
        "\n",
        "  # use the fine-tuned model for patents\n",
        "  ckpt = tf.train.latest_checkpoint(ckpt_path)\n",
        "\n",
        "  # original model released by OpenAI\n",
        "  # ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))\n",
        "\n",
        "  saver.restore(sess, ckpt)\n",
        "  while True:\n",
        "    print('Input text or \"exit\" or \"Enter\" key for unconditional sampling.....')\n",
        "    seed_text = input(\">>> \")\n",
        "    if seed_text == 'exit':\n",
        "      break\n",
        "    if seed_text == '':\n",
        "      seed_text = tag_start \n",
        "    context_tokens = enc.encode(seed_text)\n",
        "    print('top_k = %s' % top_k)\n",
        "    generate_text(seed_text, sess, output_top_k, context, nsamples, batch_size)\n",
        "    print('top_p = %s' % top_p)\n",
        "    generate_text(seed_text, sess, output_top_p, context, nsamples, batch_size)\n",
        "    print('top_kp = %s' % top_kp)\n",
        "    generate_text(seed_text, sess, output_top_kp, context, nsamples, batch_size)\n",
        "\n",
        "  print('Thank you.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:64: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:67: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.random.categorical instead.\n",
            "WARNING:tensorflow:From <ipython-input-6-63e07dd75771>:35: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/finetuned_models/checkpoint/KNGPT2P/model-1500\n",
            "Input text or \"exit\" or \"Enter\" key for unconditional sampling.....\n",
            ">>> \n",
            "top_k = 40\n",
            "end of texts not detected\n",
            "======================================== SAMPLE 1 ========================================\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "top_p = 0.9\n",
            "======================================== SAMPLE 1 ========================================\n",
            "yeerichi taatittine ebiyoomon wotta yesuusin dojii beeti woyinee iraashoo gaata boonoshi mulloon ta toommooch too' iyoona boonoshi too' iiti wodemmina' one; arooba phawuloosin dojii beeti woyinee iraashoo gaata aabeen imi boonoshi ariiyemmoyich, oohee qaaro baroona gaawataache\n",
            "================================================================================\n",
            "\n",
            "\n",
            "top_kp = 0.1\n",
            "======================================== SAMPLE 1 ========================================\n",
            "makidoonee shuuraareena' on shuurikkee iriteyaache\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Input text or \"exit\" or \"Enter\" key for unconditional sampling.....\n",
            ">>> \n",
            "top_k = 40\n",
            "======================================== SAMPLE 1 ========================================\n",
            "tunatee arooch bo' oona woddo gettite! geto wodde biriyoonaa qanaato getone\n",
            "================================================================================\n",
            "\n",
            "\n",
            "top_p = 0.9\n",
            "======================================== SAMPLE 1 ========================================\n",
            "braunoocho! no oogii beeto; manoo bulli mooyona yaachoo gaata, mooyon tunataanoon yeellii beeto noone\n",
            "================================================================================\n",
            "\n",
            "\n",
            "top_kp = 0.1\n",
            "======================================== SAMPLE 1 ========================================\n",
            "xum ikkoo qajjiti kuuphe giishittino shaggito tunatone\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Input text or \"exit\" or \"Enter\" key for unconditional sampling.....\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}